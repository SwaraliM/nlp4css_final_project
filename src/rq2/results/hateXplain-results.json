{
  "model_name": "Hate-speech-CNERG/bert-base-uncased-hatexplain-rationale-two",
  "tokenizer_name": "Hate-speech-CNERG/bert-base-uncased-hatexplain-rationale-two",
  "history": {
    "train_loss": [
      0.1726734539823669,
      0.09132442511083406,
      0.057454454146646426
    ],
    "val_loss": [],
    "train_accuracy": [
      0.9330067099797635,
      0.9667030567685589,
      0.9807886888912557
    ],
    "val_accuracy": [
      0.9518013631937683,
      0.9673807205452775,
      0.9704965920155794
    ]
  },
  "overall_metrics": {
    "accuracy": 0.9691333982473223,
    "precision": 0.9632471728594507,
    "recall": 0.9724770642201835,
    "f1": 0.9678401136248351
  },
  "subgroup_metrics": {
    "target_race_asian": {
      "accuracy": 0.9896743447180302,
      "precision": 0.993006993006993,
      "recall": 0.9888579387186629,
      "f1": 0.9909281228192603,
      "fpr": 0.009242144177449169,
      "fnr": 0.011142061281337047
    },
    "target_race_black": {
      "accuracy": 0.9754689754689755,
      "precision": 0.9762790697674418,
      "recall": 0.9840600093764651,
      "f1": 0.9801540975951436,
      "fpr": 0.038288288288288286,
      "fnr": 0.015939990623534926
    },
    "target_race_latinx": {
      "accuracy": 0.9765124555160143,
      "precision": 0.9826989619377162,
      "recall": 0.9793103448275862,
      "f1": 0.9810017271157168,
      "fpr": 0.028037383177570093,
      "fnr": 0.020689655172413793
    },
    "target_race_middle_eastern": {
      "accuracy": 0.9739413680781759,
      "precision": 0.9834983498349835,
      "recall": 0.9727965179542981,
      "f1": 0.9781181619256017,
      "fpr": 0.024350649350649352,
      "fnr": 0.02720348204570185
    },
    "target_race_native_american": {
      "accuracy": 0.9794050343249427,
      "precision": 0.9578947368421052,
      "recall": 0.9479166666666666,
      "f1": 0.9528795811518325,
      "fpr": 0.011730205278592375,
      "fnr": 0.052083333333333336
    },
    "target_race_pacific_islander": {
      "accuracy": 0.9783783783783784,
      "precision": 0.9741379310344828,
      "recall": 0.9576271186440678,
      "f1": 0.9658119658119658,
      "fpr": 0.011904761904761904,
      "fnr": 0.0423728813559322
    },
    "target_race_white": {
      "accuracy": 0.9719326383319968,
      "precision": 0.9615384615384616,
      "recall": 0.9635974304068522,
      "f1": 0.9625668449197861,
      "fpr": 0.023076923076923078,
      "fnr": 0.03640256959314775
    },
    "target_race_other": {
      "accuracy": 0.9639389736477115,
      "precision": 0.9554317548746518,
      "recall": 0.9716713881019831,
      "f1": 0.9634831460674157,
      "fpr": 0.043478260869565216,
      "fnr": 0.028328611898016998
    },
    "target_religion_atheist": {
      "accuracy": 0.9632352941176471,
      "precision": 0.9459459459459459,
      "recall": 0.9210526315789473,
      "f1": 0.9333333333333333,
      "fpr": 0.02040816326530612,
      "fnr": 0.07894736842105263
    },
    "target_religion_buddhist": {
      "accuracy": 0.9375,
      "precision": 0.9166666666666666,
      "recall": 0.9166666666666666,
      "f1": 0.9166666666666666,
      "fpr": 0.05,
      "fnr": 0.08333333333333333
    },
    "target_religion_christian": {
      "accuracy": 0.9635193133047211,
      "precision": 0.8783068783068783,
      "recall": 0.9378531073446328,
      "f1": 0.907103825136612,
      "fpr": 0.030463576158940398,
      "fnr": 0.062146892655367235
    },
    "target_religion_hindu": {
      "accuracy": 0.9562841530054644,
      "precision": 0.9523809523809523,
      "recall": 0.9230769230769231,
      "f1": 0.9375,
      "fpr": 0.025423728813559324,
      "fnr": 0.07692307692307693
    },
    "target_religion_jewish": {
      "accuracy": 0.9769427839453458,
      "precision": 0.9782608695652174,
      "recall": 0.9922822491730982,
      "f1": 0.9852216748768473,
      "fpr": 0.07575757575757576,
      "fnr": 0.007717750826901874
    },
    "target_religion_mormon": {
      "accuracy": 0.957983193277311,
      "precision": 0.9,
      "recall": 0.972972972972973,
      "f1": 0.935064935064935,
      "fpr": 0.04878048780487805,
      "fnr": 0.02702702702702703
    },
    "target_religion_muslim": {
      "accuracy": 0.9822572695909315,
      "precision": 0.9850597609561753,
      "recall": 0.9792079207920792,
      "f1": 0.9821251241310824,
      "fpr": 0.014720314033366046,
      "fnr": 0.020792079207920793
    },
    "target_religion_other": {
      "accuracy": 0.978328173374613,
      "precision": 0.9770114942528736,
      "recall": 0.9444444444444444,
      "f1": 0.96045197740113,
      "fpr": 0.008583690987124463,
      "fnr": 0.05555555555555555
    },
    "target_origin_immigrant": {
      "accuracy": 0.9668335419274092,
      "precision": 0.9797891036906854,
      "recall": 0.9737991266375546,
      "f1": 0.9767849321068769,
      "fpr": 0.05077262693156733,
      "fnr": 0.026200873362445413
    },
    "target_origin_migrant_worker": {
      "accuracy": 0.9630541871921182,
      "precision": 0.9793103448275862,
      "recall": 0.9692832764505119,
      "f1": 0.9742710120068611,
      "fpr": 0.05309734513274336,
      "fnr": 0.030716723549488054
    },
    "target_origin_specific_country": {
      "accuracy": 0.9746359793330202,
      "precision": 0.9780821917808219,
      "recall": 0.9727520435967303,
      "f1": 0.9754098360655737,
      "fpr": 0.023346303501945526,
      "fnr": 0.027247956403269755
    },
    "target_origin_undocumented": {
      "accuracy": 0.971,
      "precision": 0.9835616438356164,
      "recall": 0.9768707482993197,
      "f1": 0.9802047781569966,
      "fpr": 0.045283018867924525,
      "fnr": 0.02312925170068027
    },
    "target_origin_other": {
      "accuracy": 0.9738219895287958,
      "precision": 0.981042654028436,
      "recall": 0.971830985915493,
      "f1": 0.9764150943396226,
      "fpr": 0.023668639053254437,
      "fnr": 0.028169014084507043
    },
    "target_gender_men": {
      "accuracy": 0.9420180722891566,
      "precision": 0.8888888888888888,
      "recall": 0.9400921658986175,
      "f1": 0.9137737961926092,
      "fpr": 0.05704697986577181,
      "fnr": 0.059907834101382486
    },
    "target_gender_non_binary": {
      "accuracy": 0.9666666666666667,
      "precision": 0.9324324324324325,
      "recall": 0.9078947368421053,
      "f1": 0.92,
      "fpr": 0.017605633802816902,
      "fnr": 0.09210526315789473
    },
    "target_gender_transgender_men": {
      "accuracy": 0.964221824686941,
      "precision": 0.9210526315789473,
      "recall": 0.9051724137931034,
      "f1": 0.9130434782608695,
      "fpr": 0.020316027088036117,
      "fnr": 0.09482758620689655
    },
    "target_gender_transgender_unspecified": {
      "accuracy": 0.9870283018867925,
      "precision": 0.9425287356321839,
      "recall": 0.9318181818181818,
      "f1": 0.9371428571428572,
      "fpr": 0.006578947368421052,
      "fnr": 0.06818181818181818
    },
    "target_gender_transgender_women": {
      "accuracy": 0.9633620689655172,
      "precision": 0.9223300970873787,
      "recall": 0.9134615384615384,
      "f1": 0.9178743961352657,
      "fpr": 0.022222222222222223,
      "fnr": 0.08653846153846154
    },
    "target_gender_women": {
      "accuracy": 0.9532570860268523,
      "precision": 0.9482233502538071,
      "recall": 0.9559877175025588,
      "f1": 0.9520897043832823,
      "fpr": 0.04932301740812379,
      "fnr": 0.044012282497441144
    },
    "target_gender_other": {
      "accuracy": 0.9174311926605505,
      "precision": 0.9230769230769231,
      "recall": 0.9056603773584906,
      "f1": 0.9142857142857143,
      "fpr": 0.07142857142857142,
      "fnr": 0.09433962264150944
    },
    "target_sexuality_bisexual": {
      "accuracy": 0.9736842105263158,
      "precision": 0.9487179487179487,
      "recall": 0.9418181818181818,
      "f1": 0.9452554744525548,
      "fpr": 0.016184971098265895,
      "fnr": 0.05818181818181818
    },
    "target_sexuality_gay": {
      "accuracy": 0.9712258573117856,
      "precision": 0.9633507853403142,
      "recall": 0.9726872246696036,
      "f1": 0.9679964927663306,
      "fpr": 0.029957203994293864,
      "fnr": 0.027312775330396475
    },
    "target_sexuality_lesbian": {
      "accuracy": 0.9675213675213675,
      "precision": 0.9559322033898305,
      "recall": 0.9185667752442996,
      "f1": 0.9368770764119602,
      "fpr": 0.015063731170336037,
      "fnr": 0.08143322475570032
    },
    "target_sexuality_straight": {
      "accuracy": 0.9388335704125178,
      "precision": 0.8884120171673819,
      "recall": 0.9241071428571429,
      "f1": 0.9059080962800875,
      "fpr": 0.054279749478079335,
      "fnr": 0.07589285714285714
    },
    "target_sexuality_other": {
      "accuracy": 0.9625468164794008,
      "precision": 0.881578947368421,
      "recall": 0.8589743589743589,
      "f1": 0.8701298701298701,
      "fpr": 0.019736842105263157,
      "fnr": 0.14102564102564102
    },
    "target_age_children": {
      "accuracy": 0.9081632653061225,
      "precision": 0.8529411764705882,
      "recall": 0.8787878787878788,
      "f1": 0.8656716417910447,
      "fpr": 0.07692307692307693,
      "fnr": 0.12121212121212122
    },
    "target_age_teenagers": {
      "accuracy": 0.9158878504672897,
      "precision": 0.8333333333333334,
      "recall": 0.9459459459459459,
      "f1": 0.8860759493670886,
      "fpr": 0.1,
      "fnr": 0.05405405405405406
    },
    "target_age_young_adults": {
      "accuracy": 0.9115646258503401,
      "precision": 0.8846153846153846,
      "recall": 0.8679245283018868,
      "f1": 0.8761904761904762,
      "fpr": 0.06382978723404255,
      "fnr": 0.1320754716981132
    },
    "target_age_middle_aged": {
      "accuracy": 0.935064935064935,
      "precision": 0.96875,
      "recall": 0.8857142857142857,
      "f1": 0.9253731343283582,
      "fpr": 0.023809523809523808,
      "fnr": 0.11428571428571428
    },
    "target_age_seniors": {
      "accuracy": 1.0,
      "precision": 1.0,
      "recall": 1.0,
      "f1": 1.0,
      "fpr": 0.0,
      "fnr": 0.0
    },
    "target_age_other": {
      "accuracy": 0.8666666666666667,
      "precision": 0.8888888888888888,
      "recall": 0.8888888888888888,
      "f1": 0.8888888888888888,
      "fpr": 0.16666666666666666,
      "fnr": 0.1111111111111111
    },
    "target_disability_physical": {
      "accuracy": 0.9869281045751634,
      "precision": 0.9925373134328358,
      "recall": 0.9925373134328358,
      "f1": 0.9925373134328358,
      "fpr": 0.05263157894736842,
      "fnr": 0.007462686567164179
    },
    "target_disability_cognitive": {
      "accuracy": 0.9705882352941176,
      "precision": 0.9956896551724138,
      "recall": 0.9705882352941176,
      "f1": 0.9829787234042553,
      "fpr": 0.029411764705882353,
      "fnr": 0.029411764705882353
    },
    "target_disability_neurological": {
      "accuracy": 0.9305555555555556,
      "precision": 0.9777777777777777,
      "recall": 0.9166666666666666,
      "f1": 0.946236559139785,
      "fpr": 0.041666666666666664,
      "fnr": 0.08333333333333333
    },
    "target_disability_visually_impaired": {
      "accuracy": 0.8888888888888888,
      "precision": 0.9285714285714286,
      "recall": 0.9285714285714286,
      "f1": 0.9285714285714286,
      "fpr": 0.25,
      "fnr": 0.07142857142857142
    },
    "target_disability_hearing_impaired": {
      "accuracy": 0.8888888888888888,
      "precision": 0.9230769230769231,
      "recall": 0.9230769230769231,
      "f1": 0.9230769230769231,
      "fpr": 0.2,
      "fnr": 0.07692307692307693
    },
    "target_disability_unspecific": {
      "accuracy": 0.9637681159420289,
      "precision": 0.9809523809523809,
      "recall": 0.9716981132075472,
      "f1": 0.976303317535545,
      "fpr": 0.0625,
      "fnr": 0.02830188679245283
    },
    "target_disability_other": {
      "accuracy": 0.9310344827586207,
      "precision": 1.0,
      "recall": 0.9090909090909091,
      "f1": 0.9523809523809523,
      "fpr": 0.0,
      "fnr": 0.09090909090909091
    }
  },
  "fairness_metrics": {
    "f1_disparity": 0.034999736084427066,
    "fpr_disparity": 0.04832091333723236,
    "fnr_disparity": 0.03552833015926115
  }
}